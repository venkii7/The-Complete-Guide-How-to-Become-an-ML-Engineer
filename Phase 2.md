## Phase 2: Build Implementation Skills with Karpathy

If 3Blue1Brown gives you the intuition, Andrej Karpathy gives you the implementation.
Karpathy was the founding member of OpenAI and former Senior Director of AI at Tesla. He's one of the most influential ML educators alive.

This course is how you learn to actually build things.

*Prerequisites: You should be comfortable with Python. You need basic math—derivatives, gaussian distributions, that kind of thing. If you've finished the 3Blue1Brown series, you're ready.*

### Time investment: ~30-40 hours total
## Video 1: Building micrograd
Link: https://youtu.be/VMj-3S1tku0

Duration: 2 hours 25 minutes

*This is where it gets real.
Karpathy builds an entire automatic differentiation engine from scratch. From nothing to a working neural network that can learn—in pure Python, no libraries.
This video is legendary in the ML community. By the end, you'll understand backpropagation at a level most ML practitioners never reach. You'll have built it yourself.*

*Take your time with this one. Type every line of code. Build micrograd yourself. This is the foundation everything else builds on.*

## Video 2: Building makemore
Link: https://youtu.be/PaCmpygFfXo

Duration: 1 hour 57 minutes

*Now you build a character-level language model. Given some text, the model learns to generate more text like it.
This introduces you to language modeling—the core task behind LLMs. You'll implement it from scratch, understanding every piece.
By the end, you'll have a model that can generate fake names, fake words, fake whatever you train it on. More importantly, you'll understand how it does it.* 

## Video 3: Building makemore Part 2: MLP
Link: https://youtu.be/TCH_1BHY58I 

Duration: 1 hour 15 minutes

*You rebuild makemore using a multi-layer perceptron (MLP). This is the architecture described in a famous Bengio et al. paper.
Now you're implementing ideas from actual research papers. This is what real ML engineering looks like—reading papers and implementing them.*
Video 4: Building makemore Part 3: Activations & Gradients, BatchNorm
Link: https://youtu.be/P6sfmUTpUmc
Duration: 1 hour 55 minutes
This video dives into the internals of neural network training. Activations, gradients, why things go wrong, and how to fix them.
Batch normalization is one of those techniques that makes training actually work. You'll understand why it exists and how to implement it.
